

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Finetune API Reference &mdash; finetune 0.8.2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="finetune 0.8.2 documentation" href="index.html"/>
        <link rel="next" title="Cached Prediction" href="cachedpredict.html"/>
        <link rel="prev" title="Resource Management" href="resource.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> finetune
          

          
          </a>

          
            
            
              <div class="version">
                0.8.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html#docker">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="basemodels.html">Base Models (e.g. BERT, GPT2, RoBERTa, DistilBERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Finetune Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasetloading.html">Dataset Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="serializing.html">Saving and Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="config.html">Model Configuration Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="resource.html">Resource Management</a></li>
</ul>
<p class="caption"><span class="caption-text">Special Features</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Finetune API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#classifier"><code class="docutils literal notranslate"><span class="pre">Classifier</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#regressor"><code class="docutils literal notranslate"><span class="pre">Regressor</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#sequencelabeler"><code class="docutils literal notranslate"><span class="pre">SequenceLabeler</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#association"><code class="docutils literal notranslate"><span class="pre">Association</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#comparison"><code class="docutils literal notranslate"><span class="pre">Comparison</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#multifieldclassifier"><code class="docutils literal notranslate"><span class="pre">MultiFieldClassifier</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#multifieldregressor"><code class="docutils literal notranslate"><span class="pre">MultiFieldRegressor</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#multilabelclassifier"><code class="docutils literal notranslate"><span class="pre">MultiLabelClassifier</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#ordinalregressor"><code class="docutils literal notranslate"><span class="pre">OrdinalRegressor</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#comparisonordinalregressor"><code class="docutils literal notranslate"><span class="pre">ComparisonOrdinalRegressor</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#multitask"><code class="docutils literal notranslate"><span class="pre">MultiTask</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#languagemodel"><code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploymentmodel"><code class="docutils literal notranslate"><span class="pre">DeploymentModel</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cachedpredict.html">Cached Prediction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chunk.html">Process Long Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="sequencelabeler.html">SequenceLabeler Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="adapter.html">Using Adapters and the DeploymentModel class</a></li>
<li class="toctree-l1"><a class="reference internal" href="auxiliary.html">Using Auxiliary Info</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">finetune</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Finetune API Reference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="finetune-api-reference">
<h1>Finetune API Reference<a class="headerlink" href="#finetune-api-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="classifier">
<h2><code class="docutils literal notranslate"><span class="pre">Classifier</span></code><a class="headerlink" href="#classifier" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.Classifier">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Classifier</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a single document into 1 of N categories.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Classifier.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – integer or string-valued class labels.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Classifier.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Classifier.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Classifier.save" title="finetune.Classifier.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>probas=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
<p>Chunk idx for prediction.  Dividers at <cite>step_size</cite> increments.
[  1  |  1  |  2  |  3  |  3  ]</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="regressor">
<h2><code class="docutils literal notranslate"><span class="pre">Regressor</span></code><a class="headerlink" href="#regressor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.Regressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Regressor</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Regresses one or more floating point values given a single document.</p>
<p>For a full list of configuration options, see <cite>finetune.config</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A config object generated by <cite>finetune.config.get_config</cite> or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Regressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Regressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Regressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Regressor.save" title="finetune.Regressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequencelabeler">
<h2><code class="docutils literal notranslate"><span class="pre">SequenceLabeler</span></code><a class="headerlink" href="#sequencelabeler" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.SequenceLabeler">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">SequenceLabeler</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler" title="Permalink to this definition">¶</a></dt>
<dd><p>Labels each token in a sequence as belonging to 1 of N token classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.SequenceLabeler.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> – An iterable of lists or array of text, shape [batch, n_inputs, tokens]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.SequenceLabeler.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.SequenceLabeler.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.SequenceLabeler.save" title="finetune.SequenceLabeler.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>per_token=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> – A list / array of text, shape [batch]</li>
<li><strong>per_token</strong> – If True, return raw probabilities and labels on a per token basis</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">list of class labels.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A list / array of text, shape [batch]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="association">
<h2><code class="docutils literal notranslate"><span class="pre">Association</span></code><a class="headerlink" href="#association" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.Association">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Association</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/association.html#Association"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Association" title="Permalink to this definition">¶</a></dt>
<dd><p>Labels each token in a sequence as belonging to 1 of N token classes and then builds a set of edges
between the labeled edges.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Association.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/association.html#Association.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Association.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> – An iterable of lists or array of text, shape [batch, n_inputs, tokens]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/association.html#Association.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Association.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>Xs</strong> – A list of strings.</li>
<li><strong>Y</strong> – A list of labels of the same format as sequence labeling but with an option al additional field</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>of the form:
<a href="#id7"><span class="problematic" id="id8">``</span></a><a href="#id9"><span class="problematic" id="id10">`</span></a></p>
<blockquote>
<div><dl class="docutils">
<dt>{</dt>
<dd><p class="first">…
“association”:{</p>
<blockquote class="last">
<div>“index”: a,
“relationship”: relationship_name</div></blockquote>
</dd>
</dl>
</div></blockquote>
<p><a href="#id11"><span class="problematic" id="id12">``</span></a>`
where index is the index of the relationship target into the label list and relationship_name is the type of
the relationship.</p>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Association.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Association.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Association.save" title="finetune.Association.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/association.html#Association.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Association.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A list / array of text, shape [batch]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/association.html#Association.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Association.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A list / array of text, shape [batch]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Association.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Association.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="comparison">
<h2><code class="docutils literal notranslate"><span class="pre">Comparison</span></code><a class="headerlink" href="#comparison" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.Comparison">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Comparison</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison" title="Permalink to this definition">¶</a></dt>
<dd><p>Compares two documents to solve a classification task.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Comparison.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – integer or string-valued class labels.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Comparison.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Comparison.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Comparison.save" title="finetune.Comparison.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multifieldclassifier">
<h2><code class="docutils literal notranslate"><span class="pre">MultiFieldClassifier</span></code><a class="headerlink" href="#multifieldclassifier" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.MultiFieldClassifier">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiFieldClassifier</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a set of documents into 1 of N classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiFieldClassifier.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</li>
<li><strong>Y</strong> – integer or string-valued class labels. It is necessary for the items of Y to be sortable.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldClassifier.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldClassifier.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiFieldClassifier.save" title="finetune.MultiFieldClassifier.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from X2 class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multifieldregressor">
<h2><code class="docutils literal notranslate"><span class="pre">MultiFieldRegressor</span></code><a class="headerlink" href="#multifieldregressor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.MultiFieldRegressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiFieldRegressor</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Regresses one or more floating point values given a set of documents per example.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiFieldRegressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldRegressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldRegressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiFieldRegressor.save" title="finetune.MultiFieldRegressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from X2 class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multilabelclassifier">
<h2><code class="docutils literal notranslate"><span class="pre">MultiLabelClassifier</span></code><a class="headerlink" href="#multilabelclassifier" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.MultiLabelClassifier">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiLabelClassifier</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a single document into upto N of N categories.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiLabelClassifier.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – A list of lists containing labels for the corresponding X</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiLabelClassifier.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiLabelClassifier.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiLabelClassifier.save" title="finetune.MultiLabelClassifier.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>threshold=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="ordinalregressor">
<h2><code class="docutils literal notranslate"><span class="pre">OrdinalRegressor</span></code><a class="headerlink" href="#ordinalregressor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.OrdinalRegressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">OrdinalRegressor</code><span class="sig-paren">(</span><em>shared_threshold_weights=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a document into two or more ordered categories.</p>
<p>For a full list of configuration options, see <cite>finetune.config</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A config object generated by <cite>finetune.config.get_config</cite> or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.OrdinalRegressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.OrdinalRegressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.OrdinalRegressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.OrdinalRegressor.save" title="finetune.OrdinalRegressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="comparisonordinalregressor">
<h2><code class="docutils literal notranslate"><span class="pre">ComparisonOrdinalRegressor</span></code><a class="headerlink" href="#comparisonordinalregressor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.ComparisonOrdinalRegressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">ComparisonOrdinalRegressor</code><span class="sig-paren">(</span><em>shared_threshold_weights=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#ComparisonOrdinalRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Compares two documents and classifies into two or more ordered categories.</p>
<p>For a full list of configuration options, see <cite>finetune.config</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A config object generated by <cite>finetune.config.get_config</cite> or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.ComparisonOrdinalRegressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.ComparisonOrdinalRegressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.ComparisonOrdinalRegressor.save" title="finetune.ComparisonOrdinalRegressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#ComparisonOrdinalRegressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a floating point prediction determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of floats, shape [batch]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multitask">
<h2><code class="docutils literal notranslate"><span class="pre">MultiTask</span></code><a class="headerlink" href="#multitask" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.MultiTask">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiTask</code><span class="sig-paren">(</span><em>tasks</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask" title="Permalink to this definition">¶</a></dt>
<dd><p>Target model for multi task learning. The approach used is to sample mini-batches from each task proportional to
the size of the task for each dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tasks</strong> – A dictionary of pairs mapping string task names to model classes.
eg. <cite>{“sst”: Classifier, “ner”: SequenceLabeler}</cite></li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override. Note: The same config is used for each base task.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiTask.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.cached_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
<p>Not supported for MultiTask.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs featurization on the trained model for any of the tasks the model was trained for. Input and output formats  
are the same as for each of the individial tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A dictionary mapping from task name to data, in the format required by the task type.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A dictionary mapping from task name to the features for that task.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> – A dictionary mapping from task name to inputs in the same format required for each of the models.</li>
<li><strong>Y</strong> – A dictionary mapping from task name to targets in the same format required for each of the models.</li>
<li><strong>batch_size</strong> – Number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiTask.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiTask.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiTask.save" title="finetune.MultiTask.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs inference on the trained model for any of the tasks the model was trained for. Input and output formats
are the same as for each of the individial tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A dictionary mapping from task name to data, in the format required by the task type.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A dictionary mapping from task name to the predictions for that task.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs probability inference on the trained model for any of the tasks the model was trained for. Falls back
to normal predict when probabilities are not available for a task, eg Regression.</p>
<p>Input and output formats are the same as for each of the individial tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A dictionary mapping from task name to data, in the format required by the task type.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A dictionary mapping from task name to the predictions for that task.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="languagemodel">
<h2><code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code><a class="headerlink" href="#languagemodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.LanguageModel">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">LanguageModel</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/language_model.html#LanguageModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.LanguageModel" title="Permalink to this definition">¶</a></dt>
<dd><p>A Language Model for Finetune</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.LanguageModel.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/language_model.html#LanguageModel.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.LanguageModel.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – Not used.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.LanguageModel.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.LanguageModel.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.LanguageModel.save" title="finetune.LanguageModel.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/language_model.html#LanguageModel.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.LanguageModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Perplexities of each of the input sentences.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/language_model.html#LanguageModel.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.LanguageModel.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.LanguageModel.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.LanguageModel.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="deploymentmodel">
<h2><code class="docutils literal notranslate"><span class="pre">DeploymentModel</span></code><a class="headerlink" href="#deploymentmodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="finetune.DeploymentModel">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">DeploymentModel</code><span class="sig-paren">(</span><em>featurizer=None</em>, <em>auxiliary_info=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements inference in arbitrary tasks in a cached manner by loading weights efficiently, allowing for quick interchanging of
weights while avoiding slow graph recompilation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="config.html#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.DeploymentModel.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.cached_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.context_span_to_label_span">
<code class="descname">context_span_to_label_span</code><span class="sig-paren">(</span><em>batch_context_spans</em>, <em>batch_text_chunks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.DeploymentModel.context_span_to_label_span" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy relevant context spans into each corresponding label span as denoted by batch_text_chunks</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.create_base_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="index.html#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.featurize_sequence">
<code class="descname">featurize_sequence</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.DeploymentModel.featurize_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Base method to get raw token-level features out of the model.
These features are the same features that are fed into the target_model.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.fill_in_context_gaps">
<code class="descname">fill_in_context_gaps</code><span class="sig-paren">(</span><em>X</em>, <em>context</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.DeploymentModel.fill_in_context_gaps" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all tokens are covered by context; if they are not, fill in gaps with the default style as given from the user</p>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.DeploymentModel.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.finetune_grid_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.DeploymentModel.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.finetune_grid_search_cv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.generate_text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>path</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.DeploymentModel.save" title="finetune.DeploymentModel.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.load_custom_model">
<code class="descname">load_custom_model</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.load_custom_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.load_custom_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load in target model, and either adapters or entire featurizer from file. Must be called after load_featurizer.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.load_featurizer">
<code class="descname">load_featurizer</code><span class="sig-paren">(</span><em>override=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.load_featurizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.load_featurizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs graph compilation of the featurizer, saving most compilation overhead from occurring at predict time. Should
be called after initialization but BEFORE any calls to load_custom_model or predict.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exclude_target=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs inference using the weights and targets from the model in filepath used for load_custom_model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/deployment_model.html#DeploymentModel.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.DeploymentModel.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.DeploymentModel.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.DeploymentModel.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cachedpredict.html" class="btn btn-neutral float-right" title="Cached Prediction" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="resource.html" class="btn btn-neutral" title="Resource Management" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Madison May, Ben Townsend, Lily Zhang, Matthew Bayer.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.8.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>