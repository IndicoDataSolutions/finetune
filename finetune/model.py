import os
import random
import json
import warnings

import pandas as pd
import tensorflow as tf
import numpy as np
from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder

from functools import partial
from finetune.encoding import TextEncoder
from finetune.optimizers import AdamWeightDecay
from finetune.utils import find_trainable_variables, shape_list, assign_to_gpu, average_grads, iter_data
from finetune.config import MAX_LENGTH, BATCH_SIZE, WEIGHT_STDDEV, N_EPOCHS, CLF_P_DROP, SEED, N_GPUS, WEIGHT_STDDEV, EMBED_P_DROP, RESID_P_DROP, N_HEADS, N_LAYER, ATTN_P_DROP, ACT_FN, LM_LOSS_COEF, LR, B1, B2, L2_REG, VECTOR_L2, EPSILON,LR_SCHEDULE, MAX_GRAD_NORM, LM_LOSS_COEF, LR_WARMUP
from finetune.train import block, dropout, embed, lr_schedules

SHAPES_PATH = os.path.join(os.path.dirname(__file__), '..', 'model', 'params_shapes.json')
PARAM_PATH = os.path.join(os.path.dirname(__file__), '..', 'model', 'params_{}.npy')
N_EMBED = 768


def clf(x, ny, w_init=tf.random_normal_initializer(stddev=WEIGHT_STDDEV), b_init=tf.constant_initializer(0), train=False):
    with tf.variable_scope('clf'):
        nx = shape_list(x)[-1]
        w = tf.get_variable("w", [nx, ny], initializer=w_init)
        b = tf.get_variable("b", [ny], initializer=b_init)
        return tf.matmul(x, w) + b

def model(X, M, Y, n_classes, encoder, train=False, reuse=None, max_length=MAX_LENGTH):
    with tf.variable_scope('model', reuse=reuse):
        we = tf.get_variable("we", [encoder.vocab_size + max_length, N_EMBED], initializer=tf.random_normal_initializer(stddev=WEIGHT_STDDEV))
        we = dropout(we, EMBED_P_DROP, train)

        X = tf.reshape(X, [-1, max_length, 2])
        M = tf.reshape(M, [-1, max_length])

        h = embed(X, we)
        for layer in range(N_LAYER):
            h = block(h, N_HEADS, ACT_FN, RESID_P_DROP, ATTN_P_DROP,'h%d'%layer, train=train, scale=True)

        # language model ignores last hidden state because we don't have a target
        lm_h = tf.reshape(h[:, :-1], [-1, N_EMBED]) # [batch, seq_len, embed] --> [batch * seq_len, embed]
        lm_logits = tf.matmul(lm_h, we, transpose_b=True) # tied weights
        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=lm_logits,
            labels=tf.reshape(X[:, 1:, 0], [-1])
        )

        lm_losses = tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1] - 1])
        lm_losses = tf.reduce_sum(lm_losses * M[:, 1:], 1) / tf.reduce_sum(M[:, 1:], 1)

        # Use hidden state at classifier token as input to final proj. + softmax
        clf_h = tf.reshape(h, [-1, N_EMBED]) # [batch * seq_len, embed]
        clf_token = encoder['_classify_']
        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), tf.float32), 1), tf.int32)
        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32) * max_length + pool_idx)

        clf_h = tf.reshape(clf_h, [-1, N_EMBED])
        if train and CLF_P_DROP > 0:
            clf_h = tf.nn.dropout(clf_h, keep_prob=(1 - CLF_P_DROP))
        clf_logits = clf(clf_h, n_classes, train=train)

        clf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=clf_logits, labels=Y)
        return clf_logits, clf_losses, lm_losses, clf_h


class LanguageModelClassifier(object):

    def __init__(self, max_length=MAX_LENGTH, *args, **kwargs):
        # ensure results are reproducible
        random.seed(SEED)
        np.random.seed(SEED)
        tf.set_random_seed(SEED)

        self.encoder = TextEncoder()
        self.label_encoder = LabelEncoder()

        # tf placeholders
        self.X = tf.placeholder(tf.int32,   [None, max_length, 2]) # token idxs (BPE embedding + positional)
        self.M = tf.placeholder(tf.float32, [None, max_length])    # sequence mask
        self.Y = tf.placeholder(tf.int32,   [None])                # classification targets

        # symbolic ops
        self.logits    = None # classification logits
        self.clf_loss  = None # cross-entropy loss
        self.lm_losses = None # language modeling losses
        self.train     = None # gradient + parameter update
        self.features  = None # hidden representation fed to classifier

    def finetune(self, X, Y, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
        """
        X: List / array of text
        Y: Class labels
        """
        token_idxs = self.encoder.encode_for_classification(X, max_length=max_length)
        train_x, train_mask = self._array_format(token_idxs)
        n_batch_train = BATCH_SIZE * N_GPUS
        n_updates_total = (len(Y) // n_batch_train) * N_EPOCHS
        Y = self.label_encoder.fit_transform(Y)
        self.n_classes = len(self.label_encoder.classes_)

        self._build_model(self.X, self.M, self.Y, n_updates_total=n_updates_total, n_classes=self.n_classes)
        self._load_saved_params()

        dataset = shuffle(train_x, train_mask, Y, random_state=np.random)

        best_score = 0
        for i in range(N_EPOCHS):
            for xmb, mmb, ymb in iter_data(*dataset, n_batch=n_batch_train, truncate=True, verbose=True):
                cost, _ = self.sess.run([self.clf_loss, self.train_op], {self.X: xmb, self.M: mmb, self.Y: ymb})

    def fit(self, *args, **kwargs):
        # Alias for finetune
        return self.finetune(*args, **kwargs)

    def predict(self, X, max_length=None):
        predictions = []
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore")
            for xmb, mmb in self._infer_prep(X, max_length):
                class_idx = self.sess.run(self.predict_op, {self.X: xmb, self.M: mmb})
                features = self.sess.run(self.features, {self.X: xmb, self.M: mmb})
                class_labels = self.label_encoder.inverse_transform(class_idx)
                predictions.append(class_labels)
        return np.concatenate(predictions)

    def predict_proba(self, X, max_length=None):
        predictions = []
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore")
            for xmb, mmb in self._infer_prep(X, max_length):
                probas = self.sess.run(self.predict_proba_op, {self.X: xmb, self.M: mmb})
                classes = self.label_encoder.classes_
                predictions.extend([
                    dict(zip(classes, proba)) for proba in probas
                ])
        return np.asarray(predictions)

    def featurize(self, X, max_length=None):
        """
        Embed inputs in learned feature space
        TODO: enable featurization without finetuning (using pre-trained model only)
        """
        features = []
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore")
            for xmb, mmb in self._infer_prep(X, max_length):
                feature_batch = self.sess.run(self.features, {self.X: xmb, self.M: mmb})
                features.append(feature_batch)
        return np.concatenate(features)

    def transform(self, *args, **kwargs):
        return self.featurize(*args, **kwargs)

    def _infer_prep(self, X, max_length=None):
        max_length = max_length or MAX_LENGTH
        token_idxs = self.encoder.encode_for_classification(X, max_length=max_length)
        infer_x, infer_mask = self._array_format(token_idxs)
        n_batch_train = BATCH_SIZE * N_GPUS
        self._build_model(self.X, self.M, self.Y, n_updates_total=0, n_classes=self.n_classes, reuse=True, train=False)
        yield from iter_data(infer_x, infer_mask, n_batch=n_batch_train, truncate=False, verbose=True)

    def _array_format(self, token_idxs, max_length=MAX_LENGTH):
        """
        Returns numpy array of token idxs and corresponding mask
        Returned `x` array contains two channels:
            0: byte-pair encoding embedding
            1: positional embedding
        """
        n = len(token_idxs)
        seq_lengths = [len(x) for x in token_idxs]
        x    = np.zeros((n, max_length, 2), dtype=np.int32)
        mask = np.zeros((n, max_length), dtype=np.float32)
        for i, seq_length in enumerate(seq_lengths):
            # BPE embedding
            x[i, :seq_length, 0] = token_idxs[i]
            # masking: value of 1 means "consider this in cross-entropy LM loss"
            mask[i, 1:seq_length] = 1
        # positional_embeddings
        x[:, :, 1] = np.arange(self.encoder.vocab_size, self.encoder.vocab_size + max_length)
        return x, mask

    def _build_model(self, X, M, Y, n_updates_total, n_classes, reuse=None, train=True):
        """
        Finetune language model on text inputs
        """
        gpu_ops = []
        gpu_grads = []
        X = tf.split(X, N_GPUS, 0)
        M = tf.split(M, N_GPUS, 0)
        Y = tf.split(Y, N_GPUS, 0)
        for i, (splitX, splitM, splitY) in enumerate(zip(X, M, Y)):
            do_reuse = True if i > 0 else reuse

            device = tf.device(assign_to_gpu(i, "/gpu:0"))
            scope = tf.variable_scope(tf.get_variable_scope(), reuse=do_reuse)
            with device, scope:
                clf_logits, clf_losses, lm_losses, features = model(splitX, splitM, splitY, n_classes=n_classes, encoder=self.encoder, train=train, reuse=do_reuse)
                if LM_LOSS_COEF > 0:
                    train_loss = tf.reduce_mean(clf_losses) + LM_LOSS_COEF * tf.reduce_mean(lm_losses)
                else:
                    train_loss = tf.reduce_mean(clf_losses)
                params = find_trainable_variables("model")
                grads = tf.gradients(train_loss, params)
                grads = list(zip(grads, params))
                gpu_grads.append(grads)
                gpu_ops.append([clf_logits, clf_losses, lm_losses, features])

        self.logits, self.clf_losses, self.lm_losses, self.features = [tf.concat(op, 0) for op in zip(*gpu_ops)]
        self.predict_op = tf.argmax(self.logits, -1)
        self.predict_proba_op = tf.nn.softmax(self.logits, -1)
        grads = average_grads(gpu_grads)
        grads = [g for g, p in grads]
        self.train_op = AdamWeightDecay(
            params=params,
            grads=grads,
            lr=LR,
            schedule=partial(lr_schedules[LR_SCHEDULE], warmup=LR_WARMUP),
            t_total=n_updates_total,
            l2=L2_REG,
            max_grad_norm=MAX_GRAD_NORM,
            vector_l2=VECTOR_L2,
            b1=B1,
            b2=B2,
            e=EPSILON
        )
        self.clf_loss = tf.reduce_mean(self.clf_losses)

    def _load_saved_params(self, max_length=MAX_LENGTH):
        """
        Load serialized model parameters into tf Tensors
        """
        pretrained_params = find_trainable_variables('model', exclude='model/clf')
        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))
        self.sess.run(tf.global_variables_initializer())

        shapes = json.load(open('model/params_shapes.json'))
        offsets = np.cumsum([np.prod(shape) for shape in shapes])
        init_params = [np.load('model/params_{}.npy'.format(n)) for n in range(10)]
        init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
        init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]
        init_params[0] = init_params[0][:max_length]
        init_params[0] = np.concatenate([init_params[1], (np.random.randn(len(self.encoder.special_tokens), N_EMBED) * WEIGHT_STDDEV).astype(np.float32), init_params[0]], 0)
        del init_params[1]
        self.sess.run([p.assign(ip) for p, ip in zip(pretrained_params, init_params)])


if __name__ == "__main__":
    df = pd.read_csv("data/AirlineNegativity.csv")
    classifier = LanguageModelClassifier()
    classifier.finetune(df.Text.values[:100], df.Target.values[:100])
    features = classifier.transform(df.Text.values[:10])
    print(features.shape)

    # print(classifier.predict(df.Text.values[:100]))
    # print(classifier.predict_proba(df.Text.values[:10]))
