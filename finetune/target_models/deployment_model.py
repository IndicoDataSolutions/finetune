import tensorflow as tf
import numpy as np
import tqdm
import math
import itertools
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils import shuffle
from collections import namedtuple
from tensorflow.python.data import Dataset

from finetune.config import get_config
from finetune.saver import Saver, InitializeHook
from finetune.base import BaseModel
from finetune.target_models.comparison import ComparisonPipeline
from finetune.target_models.sequence_labeling import SequencePipeline, SequenceLabeler
from finetune.target_models.association import AssociationPipeline
from finetune.target_models.classifier import ClassificationPipeline
from finetune.base_models import GPTModel, GPTModelSmall, BERTModelCased, GPT2Model
from finetune.model import get_model_fn, get_separate_model_fns, PredictMode
from finetune.encoding.target_encoders import OneHotLabelEncoder
from finetune.errors import FinetuneError
from finetune.input_pipeline import BasePipeline


class DeploymentPipeline(BasePipeline):

    def __init__(self, config):
        super().__init__(config)
        self.pipeline_type = None
        self.pipeline = None

    def _target_encoder(self):
        raise NotImplementedError

    def _dataset_without_targets(self, Xs, train):
        if not callable(Xs):
            Xs_fn = lambda: self.wrap_tqdm(Xs, train)
        else:
            Xs_fn = lambda: self.wrap_tqdm(Xs(), train)
        
        dataset_encoded = lambda: itertools.chain.from_iterable(map(self.get_text_token_mask, Xs_fn()))
        types, shapes = self.feed_shape_type_def()
        return Dataset.from_generator(dataset_encoded, types[0])
        
    def pipe_gen(self):
        pipelines = {'Classification':ClassificationPipeline, 'Comparison':ComparisonPipeline, 'Sequence_Labeling':SequencePipeline, 'Association':AssociationPipeline}
        while True:
            self.pipeline_type = pipelines[self.task]#to prevent instantiating the same type of pipeline repeatedly
            yield self.pipeline_type

    def get_text_token_mask(self, X):
        _ = next(self.pipe_gen())
        if type(self.pipeline) != self.pipeline_type:
            if self.pipeline_type == SequencePipeline:
                self.pipeline = next(self.pipe_gen())(self.config, multi_label=self.multi_label)
            else:
                self.pipeline = next(self.pipe_gen())(self.config)
        return self.pipeline.text_to_tokens_mask(X)

    def get_target_input_fn(self, features, batch_size=None):
        batch_size = batch_size or self.config.batch_size*4
        features = pd.DataFrame(features).to_dict('list')
        for key in features:
            features[key] = np.array(features[key])
        tf_dataset = lambda : tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)
        return tf_dataset

class TaskMode:
    SEQUENCE_LABELING = "Sequence_Labeling"
    CLASSIFICATION = "Classification"
    COMPARISON = "Comparison"
    ASSOCIATION = "Association"

class DeploymentModel(BaseModel):
    """ 
    Implements inference in arbitrary tasks in a cached manner by loading weights efficiently, allowing for quick interchanging of
    weights without slow graph recompilation.

    :param config: A :py:class:`finetune.config.Settings` object or None (for default config).
    :param \**kwargs: key-value pairs of config items to override.
    """
    def __init__(self, base_model, **kwargs):
        """
        For a full list of configuration options, see `finetune.config`.

        :param config: A config object generated by `finetune.config.get_config` or None (for default config).
        :param **kwargs: key-value pairs of config items to override.
        """
        if base_model not in [GPTModel, GPTModelSmall, BERTModelCased, GPT2Model]:
            raise FinetuneError("Selected base model not supported.")
        self.config = get_config(**kwargs)
        self.validate_config()
        self.input_pipeline = DeploymentPipeline(self.config)
        super().__init__(**kwargs)
        self.config.base_model = base_model
        self.need_to_refresh = True
        self.task = TaskMode.CLASSIFICATION
        self.input_pipeline.task = self.task

    def load_featurizer(self):
        self.featurizer_est = self.get_estimator('featurizer')
        self.predict_hooks.feat_hook.model_portion = 'whole_featurizer'
        for hook in self.predict_hooks:
            hook.need_to_refresh = True
        output = self.predict(['finetune'], exclude_target=True) #run arbitrary predict call to compile featurizer graph

    def load_trainables(self, path):
        original_model = self.saver.load(path)
        if original_model.config.adapter_size != self.config.adapter_size:
            raise FinetuneError("Model loaded from file and base_model have incompatible adapter_size in config")
        self._target_model = original_model._target_model
        self._predict_op = original_model._predict_op
        self._predict_proba_op = original_model._predict_proba_op
        self.need_to_refresh = True
        for hook in self.predict_hooks:
            hook.need_to_refresh = True
        self._data = None
        self._to_pull = 0

        self.input_pipeline.target_dim = original_model.input_pipeline.target_dim
        self.input_pipeline.label_encoder = original_model.input_pipeline.label_encoder
        self.input_pipeline.text_encoder = original_model.input_pipeline.text_encoder
        self.input_pipeline._target_encoder = original_model.input_pipeline._target_encoder
        self.input_pipeline._post_data_initialization = original_model.input_pipeline._post_data_initialization
        self.input_pipeline._format_for_inference = original_model.input_pipeline._format_for_inference
        self.input_pipeline._format_for_encoding = original_model.input_pipeline._format_for_encoding

        if isinstance(original_model.input_pipeline, SequencePipeline) :
            self.task = TaskMode.SEQUENCE_LABELING
            self.input_pipeline.multi_label = original_model.input_pipeline.multi_label
            self.multi_label = original_model.config.multi_label_sequences
            original_model.multi_label = self.multi_label
            self._initialize = original_model._initialize
        elif isinstance(original_model.input_pipeline, ComparisonPipeline):
            self.task = TaskMode.COMPARISON
        elif isinstance(original_model.input_pipeline, BasePipeline):
            self.task = TaskMode.CLASSIFICATION
        elif isinstance(original_model.input_pipeline, AssociationPipeline):
            self.task = TaskMode.ASSOCIATION
        else:
            raise FinetuneError("Invalid pipeline in loaded file.")

        self.input_pipeline.task = self.task

    def get_estimator(self, portion):
        assert portion in ['featurizer', 'target'], "Can only split model into featurizer and target."
        conf = tf.ConfigProto(
            allow_soft_placement=self.config.soft_device_placement,
            log_device_placement=self.config.log_device_placement,
        )
        conf.gpu_options.per_process_gpu_memory_fraction = (
            self.config.per_process_gpu_memory_fraction
        )
        distribute_strategy = self._distribute_strategy(self.config.visible_gpus)
        config = tf.estimator.RunConfig(
            tf_random_seed=self.config.seed,
            save_summary_steps=self.config.val_interval,
            save_checkpoints_secs=None,
            save_checkpoints_steps=None,
            # disable auto summaries
            session_config=conf,
            log_step_count_steps=100,
            train_distribute=distribute_strategy,
            keep_checkpoint_max=0
        )
        #if self.need_to_refresh:
        fn = get_separate_model_fns(
            target_model_fn=self._target_model if portion == 'target' else None, 
            predict_op=self._predict_op,
            predict_proba_op=self._predict_proba_op,
            build_target_model=self.input_pipeline.target_dim is not None,
            encoder=self.input_pipeline.text_encoder,
            target_dim=self.input_pipeline.target_dim if portion == 'target' else None,
            label_encoder=self.input_pipeline.label_encoder if portion == 'target' else None,
            saver=self.saver,
            portion=portion,
            build_attn=not isinstance(self.input_pipeline, ComparisonPipeline)
        )

        if portion == 'featurizer':
            estimator = tf.estimator.Estimator(
                model_dir=self.estimator_dir,
                model_fn=fn,
                config=config,
                params=self.config
            )

        else:
            estimator = tf.estimator.Estimator(
                model_dir=self.estimator_dir,
                model_fn=fn,
                config=config,
                params=self.config
            )

        if hasattr(self,'predict_hooks') and portion == 'featurizer':
            for hook in self.predict_hooks:
                hook.need_to_refresh = True
        elif not hasattr(self,'predict_hooks'):
            feat_hook = InitializeHook(self.saver, model_portion='featurizer')
            target_hook = InitializeHook(self.saver, model_portion='target')
            predict_hook = namedtuple('InitializeHook', 'feat_hook target_hook')
            self.predict_hooks = predict_hook(feat_hook, target_hook)
        return estimator

    def _get_input_pipeline(self):
        return self.input_pipeline

    def featurize(self, X):
        """
        Embeds inputs in learned feature space. Can be called before or after calling :meth:`finetune`.

        :param X: list or array of text to embed.
        :returns: np.array of features of shape (n_examples, embedding_size).
        """
        raise NotImplementedError

    def get_input_fn(self, gen):
        return self.input_pipeline.get_predict_input_fn(gen)

    def pipe_input_fn(self, gen, pipe):
        fn = pipe.get_predict_input_fn(gen)
        return fn()

    def _inference(self, Xs, mode=PredictMode.NORMAL, exclude_target=False):
        """
        Performs inference using the weights and targets from the model in filepath used for load_trainables. 

        :param X: list or array of text to embed.
        :returns: list of class labels.
        """
        Xs = self.input_pipeline._format_for_inference(Xs)
        self._data = Xs
        self._closed = False
        n = len(self._data)

        if self._predictions is None:
            featurizer_est = self.get_estimator('featurizer')
            self._predictions =  featurizer_est.predict(
                    input_fn=self.get_input_fn(self._data_generator), predict_keys=None, hooks=[self.predict_hooks.feat_hook], yield_single_examples=False)
            self.predict_hooks.feat_hook.model_portion = 'featurizer'

        self._clear_prediction_queue()
        
        num_batches = math.ceil(n/self.config.batch_size)
        features = [None]*n
        for i in tqdm.tqdm(range(num_batches), total=num_batches, desc="Featurization by Batch"):
            y = next(self._predictions)
            for j in range(self.config.batch_size): #this loop needed since yield_single_examples is False. In this case, n = # of predictions * batch_size
                single_example = {key:value[j] for key,value in y.items()}
                if self.config.batch_size*i + j > n-1:
                    break
                features[self.config.batch_size*i + j] = single_example

        if exclude_target: #to initialize featurizer weights in load_featurizer
            return features

        preds = None
        if features is not None:
            target_est = self.get_estimator('target')
            target_fn = self.input_pipeline.get_target_input_fn(features)
            preds = target_est.predict(
                    input_fn=target_fn, predict_keys=mode, hooks=[self.predict_hooks.target_hook])
            preds = [pred[mode] if mode else pred for pred in preds]

        if self.task != TaskMode.SEQUENCE_LABELING:
            return self.input_pipeline.label_encoder.inverse_transform(np.asarray(preds))
        else:
            return preds

    def predict(self, X, exclude_target=False):
        if self.task == TaskMode.SEQUENCE_LABELING:
            return SequenceLabeler.predict(self, X)
        else:
            return self._inference(X, exclude_target=exclude_target)

    def predict_proba(self, X):
        """
        Produces a probability distribution over classes for each example in X.

        :param X: list or array of text to embed.
        :returns: list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.
        """
        return super().predict_proba(X)

    def finetune(self, X, Y=None, batch_size=None):
        """
        :param X: list or array of text.
        :param Y: integer or string-valued class labels.
        :param batch_size: integer number of examples per batch. When N_GPUS > 1, this number
                           corresponds to the number of training examples provided to each GPU.
        """
        raise NotImplementedError


    @classmethod
    def get_eval_fn(cls):
        raise NotImplementedError

    @staticmethod
    def _target_model(config, featurizer_state, targets, n_outputs, train=False, reuse=None, **kwargs):
        raise NotImplementedError

    def _predict_op(self, logits, **kwargs):
        raise NotImplementedError

    def _predict_proba_op(self, logits, **kwargs):
        raise NotImplementedError

    def fit(self, *args, **kwargs):
        raise NotImplementedError

    def attention_weights(self, Xs):
        raise NotImplementedError

    def generate_text(self, seed_text, max_length, use_extra_toks):
        raise NotImplementedError
    
    def save(self, path):
        raise NotImplementedError

    def create_base_model(self, filename, exists_ok):
        raise NotImplementedError

    def cached_predict(self):
        raise NotImplementedError
    
    def load(cls, path, **kwargs):
        raise NotImplementedError

    def finetune_grid_search(cls, Xs, Y, *, test_size, eval_fn, probs, return_all, **kwargs):
        raise NotImplementedError

    def finetune_grid_search_cv(cls, Xs, Y, *, n_splits, test_size, eval_fn, probs,
                                return_all, **kwargs):
        raise NotImplementedError

