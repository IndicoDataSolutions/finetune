import tensorflow as tf
import numpy as np
import tqdm
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils import shuffle
from collections import namedtuple

from finetune.config import get_config
from finetune.saver import Saver, InitializeHook
from finetune.base import BaseModel
from finetune.base_models import GPTModel, GPTModelSmall, BERTModelCased, GPT2Model
from finetune.model import get_model_fn, get_separate_model_fns, PredictMode
from finetune.encoding.target_encoders import OneHotLabelEncoder
from finetune.errors import FinetuneError
from finetune.input_pipeline import BasePipeline


class DeploymentPipeline(BasePipeline):
    def _target_encoder(self):
        raise NotImplementedError


class DeploymentModel(BaseModel):
    """ 
    Classifies a single document into 1 of N categories.

    :param config: A :py:class:`finetune.config.Settings` object or None (for default config).
    :param \**kwargs: key-value pairs of config items to override.
    """
    def __init__(self, base_model, **kwargs):
        """
        For a full list of configuration options, see `finetune.config`.

        :param config: A config object generated by `finetune.config.get_config` or None (for default config).
        :param **kwargs: key-value pairs of config items to override.
        """
        if base_model not in [GPTModel, GPTModelSmall, BERTModelCased, GPT2Model]:
            raise FinetuneError("Selected base model not supported.")
        self.config = get_config(**kwargs)
        self.validate_config()
        self.input_pipeline = DeploymentPipeline(self.config)
        super().__init__(**kwargs)
        self.config.base_model = base_model
        self.need_to_refresh = True

    def load_featurizer(self):
        self.featurizer_est = self.get_estimator('featurizer')
        self.predict_hooks.feat_hook.model_portion = 'whole_featurizer'
        for hook in self.predict_hooks:
            hook.need_to_refresh = True
        output = self.predict(['finetune'], exclude_target=True) #run arbitrary predict call to compile featurizer graph
        self._clear_prediction_queue()

    def load_trainables(self, path):
        original_model = self.saver.load(path)
        if original_model.config.adapter_size != self.config.adapter_size:
            raise FinetuneError("Model loaded from file and base_model have incompatible adapter_size in config")
        self.input_pipeline = original_model.input_pipeline
        self._target_model = original_model._target_model
        self._predict_op = original_model._predict_op
        self._predict_proba_op = original_model._predict_proba_op
        self.need_to_refresh = True
        self.target_est = self.get_estimator('target')
        for hook in self.predict_hooks:
            hook.need_to_refresh = True
        self._clear_prediction_queue()

    def get_estimator(self, portion):
        assert portion in ['featurizer', 'target'], "Can only split model into featurizer and target."
        conf = tf.ConfigProto(
            allow_soft_placement=self.config.soft_device_placement,
            log_device_placement=self.config.log_device_placement,
        )
        conf.gpu_options.per_process_gpu_memory_fraction = (
            self.config.per_process_gpu_memory_fraction
        )
        distribute_strategy = self._distribute_strategy(self.config.visible_gpus)
        config = tf.estimator.RunConfig(
            tf_random_seed=self.config.seed,
            save_summary_steps=self.config.val_interval,
            save_checkpoints_secs=None,
            save_checkpoints_steps=None,
            # disable auto summaries
            session_config=conf,
            log_step_count_steps=100,
            train_distribute=distribute_strategy,
            keep_checkpoint_max=0
        )

        if self.need_to_refresh:
            fn = get_separate_model_fns(
                target_model_fn=self._target_model if portion == 'target' else None, 
                predict_op=self._predict_op,
                predict_proba_op=self._predict_proba_op,
                build_target_model=self.input_pipeline.target_dim is not None,
                encoder=self.input_pipeline.text_encoder,
                target_dim=self.input_pipeline.target_dim if portion == 'target' else None,
                label_encoder=self.input_pipeline.label_encoder if portion == 'target' else None,
                saver=self.saver,
                portion=portion
            )

        if portion == 'featurizer':
            estimator = tf.estimator.Estimator(
                model_dir=self.estimator_dir,
                model_fn=fn,
                config=config,
                params=self.config
            )

        else:
            estimator = tf.estimator.Estimator(
                model_dir=self.estimator_dir,
                model_fn=fn,
                config=config,
                params=self.config
            )

        if hasattr(self,'predict_hooks'):
            for hook in self.predict_hooks:
                hook.need_to_refresh = True
        else:
            feat_hook = InitializeHook(self.saver, model_portion='featurizer')
            target_hook = InitializeHook(self.saver, model_portion='target')
            predict_hook = namedtuple('InitializeHook', 'feat_hook target_hook')
            self.predict_hooks = predict_hook(feat_hook, target_hook)
        return estimator

    def _get_input_pipeline(self):
        return self.input_pipeline

    def featurize(self, X):
        """
        Embeds inputs in learned feature space. Can be called before or after calling :meth:`finetune`.

        :param X: list or array of text to embed.
        :returns: np.array of features of shape (n_examples, embedding_size).
        """
        raise NotImplementedError

    def predict(self, Xs, mode=PredictMode.NORMAL, exclude_target = False):
        """
        Produces a list of most likely class labels as determined by the fine-tuned model.

        :param X: list or array of text to embed.
        :returns: list of class labels.
        """
        Xs = self.input_pipeline._format_for_inference(Xs)
        self._data = Xs
        self._closed = False
        n = len(self._data)
        self.featurizer_est = self.get_estimator('featurizer')

        print(Xs)
        print(self)
        print(self.input_pipeline)
        input_fn = self.input_pipeline.get_predict_input_fn(self._data_generator)
        print(input_fn)
        if self._predictions is None:
            self._predictions =  self.featurizer_est.predict(
                    input_fn=input_fn, predict_keys=None, hooks=[self.predict_hooks.feat_hook])
            self.predict_hooks.feat_hook.model_portion = 'featurizer'
        self._clear_prediction_queue()
        
        features = [None]*n
        for i in tqdm.tqdm(range(n), total=n, desc="Featurization"):
            y = next(self._predictions)
            features[i] = y

        if exclude_target: #to initialize featurizer weights
            return features

        self.target_est = self.get_estimator('target')
        preds = None
        if features is not None:
            target_fn = self.input_pipeline.get_target_input_fn(features)
            preds = self.target_est.predict(
                    input_fn=target_fn, predict_keys=mode, hooks=[self.predict_hooks.target_hook])
            preds = [pred[mode] if mode else pred for pred in preds]

        if self._predictions is not None:
            self._clear_prediction_queue()
        print(preds)
        return self.input_pipeline.label_encoder.inverse_transform(np.asarray(preds))


    def predict_proba(self, X):
        """
        Produces a probability distribution over classes for each example in X.

        :param X: list or array of text to embed.
        :returns: list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.
        """
        return super().predict_proba(X)

    def finetune(self, X, Y=None, batch_size=None):
        """
        :param X: list or array of text.
        :param Y: integer or string-valued class labels.
        :param batch_size: integer number of examples per batch. When N_GPUS > 1, this number
                           corresponds to the number of training examples provided to each GPU.
        """
        return super().finetune(X, Y=Y, batch_size=batch_size)

    @classmethod
    def get_eval_fn(cls):
        return lambda labels, targets: np.mean(np.asarray(labels) == np.asarray(targets))

    
    @staticmethod
    def _target_model(config, featurizer_state, targets, n_outputs, train=False, reuse=None, **kwargs):
        raise NotImplementedError
    

    def _predict_op(self, logits, **kwargs):
        raise NotImplementedError

    def _predict_proba_op(self, logits, **kwargs):
        raise NotImplementedError
