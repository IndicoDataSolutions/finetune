import tensorflow as tf
import numpy as np
import tqdm
import itertools
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils import shuffle
from collections import namedtuple
from tensorflow.python.data import Dataset

from finetune.config import get_config
from finetune.saver import Saver, InitializeHook
from finetune.base import BaseModel
from finetune.target_models.comparison import ComparisonPipeline
from finetune.target_models.sequence_labeling import SequencePipeline
from finetune.target_models.association import AssociationPipeline
from finetune.target_models.classifier import ClassificationPipeline
from finetune.base_models import GPTModel, GPTModelSmall, BERTModelCased, GPT2Model
from finetune.model import get_model_fn, get_separate_model_fns, PredictMode
from finetune.encoding.target_encoders import OneHotLabelEncoder
from finetune.errors import FinetuneError
from finetune.input_pipeline import BasePipeline


class DeploymentPipeline(BasePipeline):
    def _target_encoder(self):
        raise NotImplementedError

    def _dataset_without_targets(self, Xs, train):
        if not callable(Xs):
            Xs_fn = lambda: self.wrap_tqdm(Xs, train)
        else:
            Xs_fn = lambda: self.wrap_tqdm(Xs(), train)
        
        dataset_encoded = lambda: itertools.chain.from_iterable(map(self.get_text_token_mask, Xs_fn()))
        return Dataset.from_generator(dataset_encoded, next(self.get_feed_shape_type_def())[0]) 
        
    def pipe_gen(self):
        pipelines = {'Classification':ClassificationPipeline, 'Comparison':ComparisonPipeline, 'Sequence Labeling':SequencePipeline, 'Association':AssociationPipeline}
        while True:
            print("current pipe gen task: "+ str(self.task))
            yield pipelines[self.task]

    def get_text_token_mask(self, X):
        pipe = next(self.pipe_gen())(self.config)
        return pipe.text_to_tokens_mask(X)
        #next(self.pipe_gen())(self.config).text_to_tokens_mask

    def get_feed_shape_type_def(self):
        while True:
            print('in get feed shape')
            pipe = next(self.pipe_gen())(self.config)
            types, shapes = pipe.feed_shape_type_def()
            yield types[0], shapes[0] # 0s cut out the targets

class DeploymentModel(BaseModel):
    """ 
    Implements inference in arbitrary tasks in a cached manner by loading weights efficiently, allowing for quick interchanging of
    weights without slow graph recompilation.

    :param config: A :py:class:`finetune.config.Settings` object or None (for default config).
    :param \**kwargs: key-value pairs of config items to override.
    """
    def __init__(self, base_model, **kwargs):
        """
        For a full list of configuration options, see `finetune.config`.

        :param config: A config object generated by `finetune.config.get_config` or None (for default config).
        :param **kwargs: key-value pairs of config items to override.
        """
        if base_model not in [GPTModel, GPTModelSmall, BERTModelCased, GPT2Model]:
            raise FinetuneError("Selected base model not supported.")
        self.config = get_config(**kwargs)
        self.validate_config()
        self.input_pipeline = DeploymentPipeline(self.config)
        super().__init__(**kwargs)
        self.config.base_model = base_model
        self.need_to_refresh = True
        self.task = 'Classification'
        self.input_pipeline.task = self.task

    def load_featurizer(self):
        self.featurizer_est = self.get_estimator('featurizer')
        self.predict_hooks.feat_hook.model_portion = 'whole_featurizer'
        for hook in self.predict_hooks:
            hook.need_to_refresh = True
        output = self.predict(['finetune'], exclude_target=True) #run arbitrary predict call to compile featurizer graph
        self._clear_prediction_queue()

    def load_trainables(self, path):
        original_model = self.saver.load(path)
        if original_model.config.adapter_size != self.config.adapter_size:
            raise FinetuneError("Model loaded from file and base_model have incompatible adapter_size in config")
        #self.input_pipeline = original_model.input_pipeline.__class__(self.config)
        #self._get_input_pipeline = original_model._get_input_pipeline
        #self.input_pipeline = original_model.input_pipeline
        self._target_model = original_model._target_model
        self._predict_op = original_model._predict_op
        self._predict_proba_op = original_model._predict_proba_op
        self.need_to_refresh = True
        self.target_est = self.get_estimator('target')
        for hook in self.predict_hooks:
            hook.need_to_refresh = True
        self._data = None
        self._to_pull = 0
        if isinstance(original_model.input_pipeline, SequencePipeline) :
            self.task = 'Sequence Labeling'
        elif isinstance(original_model.input_pipeline, ComparisonPipeline):
            self.task = 'Comparison'
        elif isinstance(original_model.input_pipeline, BasePipeline):
            self.task = 'Classification'
        elif isinstance(original_model.input_pipeline, AssociationPipeline):
            self.task = "Association"
        else:
            raise FinetuneError("Invalid pipeline in loaded file.")

        self.input_pipeline.task = self.task
        self.input_pipeline.target_dim = original_model.input_pipeline.target_dim
        self.input_pipeline.label_encoder = original_model.input_pipeline.label_encoder
        self.input_pipeline.text_encoder = original_model.input_pipeline.text_encoder


    def get_estimator(self, portion):
        assert portion in ['featurizer', 'target'], "Can only split model into featurizer and target."
        conf = tf.ConfigProto(
            allow_soft_placement=self.config.soft_device_placement,
            log_device_placement=self.config.log_device_placement,
        )
        conf.gpu_options.per_process_gpu_memory_fraction = (
            self.config.per_process_gpu_memory_fraction
        )
        distribute_strategy = self._distribute_strategy(self.config.visible_gpus)
        config = tf.estimator.RunConfig(
            tf_random_seed=self.config.seed,
            save_summary_steps=self.config.val_interval,
            save_checkpoints_secs=None,
            save_checkpoints_steps=None,
            # disable auto summaries
            session_config=conf,
            log_step_count_steps=100,
            train_distribute=distribute_strategy,
            keep_checkpoint_max=0
        )
        #if self.need_to_refresh:
        fn = get_separate_model_fns(
            target_model_fn=self._target_model if portion == 'target' else None, 
            predict_op=self._predict_op,
            predict_proba_op=self._predict_proba_op,
            build_target_model=self.input_pipeline.target_dim is not None,
            encoder=self.input_pipeline.text_encoder,
            target_dim=self.input_pipeline.target_dim if portion == 'target' else None,
            label_encoder=self.input_pipeline.label_encoder if portion == 'target' else None,
            saver=self.saver,
            portion=portion,
            build_attn=not isinstance(self.input_pipeline, ComparisonPipeline)
        )

        if portion == 'featurizer':
            estimator = tf.estimator.Estimator(
                model_dir=self.estimator_dir,
                model_fn=fn,
                config=config,
                params=self.config
            )

        else:
            estimator = tf.estimator.Estimator(
                model_dir=self.estimator_dir,
                model_fn=fn,
                config=config,
                params=self.config
            )

        if hasattr(self,'predict_hooks'):
            for hook in self.predict_hooks:
                hook.need_to_refresh = True
        else:
            feat_hook = InitializeHook(self.saver, model_portion='featurizer')
            target_hook = InitializeHook(self.saver, model_portion='target')
            predict_hook = namedtuple('InitializeHook', 'feat_hook target_hook')
            self.predict_hooks = predict_hook(feat_hook, target_hook)
        return estimator

    def _get_input_pipeline(self):
        return self.input_pipeline

    def featurize(self, X):
        """
        Embeds inputs in learned feature space. Can be called before or after calling :meth:`finetune`.

        :param X: list or array of text to embed.
        :returns: np.array of features of shape (n_examples, embedding_size).
        """
        raise NotImplementedError

    def get_input_fn(self, gen):
        print('in get input fn')
        return self.input_pipeline.get_predict_input_fn(gen)
    '''
    def input_fns(self):
        print('in input_fns')
        while not self._closed:
            print('yielding')
            pipelines = {'Classification':ClassificationPipeline, 'Comparison':ComparisonPipeline, 'Sequence Labeling':SequencePipeline, 'Association':AssociationPipeline}
            task = next(self._data_generator())
            pipe = pipelines[self.task](self.config)
            fn = pipe.get_predict_input_fn(self._data_generator)
            yield fn()
    
    def select_pipeline(self, gen):
        print('in select_pipeline')
        self.task = next(self._data_generator())
        #self.task = self._data.pop(0)
        #task = next(gen())
        print(self.task)
        #print(self._data)
        pipelines = {'Classification':ClassificationPipeline, 'Comparison':ComparisonPipeline, 'Sequence Labeling':SequencePipeline, 'Association':AssociationPipeline}
        pipe = pipelines[self.task](self.config)
        return lambda pipe=pipe: self.pipe_input_fn(gen, pipe)
    '''
    def pipe_input_fn(self, gen, pipe):
        fn = pipe.get_predict_input_fn(gen)
        return fn()

    def predict(self, Xs, mode=PredictMode.NORMAL, exclude_target = False):
        """
        Performs inference using the weights and targets from the model in filepath used for load_trainables. 

        :param X: list or array of text to embed.
        :returns: list of class labels.
        """
        print(Xs)
        Xs = self.input_pipeline._format_for_inference(Xs)
        self._data = Xs
        self._closed = False
        n = len(self._data)

        if self._predictions is None:
            featurizer_est = self.get_estimator('featurizer')
            self._predictions =  featurizer_est.predict(
                    input_fn=self.get_input_fn(self._data_generator), predict_keys=None, hooks=[self.predict_hooks.feat_hook], yield_single_examples=False)
            self.predict_hooks.feat_hook.model_portion = 'featurizer'

        self._clear_prediction_queue()
        
        features = [None]*n
        for i in tqdm.tqdm(range(n), total=n, desc="Featurization"):
            y = next(self._predictions)
            for key in y:
                print(np.shape(y[key]))
            features[i] = y

        if exclude_target: #to initialize featurizer weights
            return features

        preds = None
        if features is not None:
            target_est = self.get_estimator('target')
            target_fn = self.input_pipeline.get_target_input_fn(features)
            preds = target_est.predict(
                    input_fn=target_fn, predict_keys=mode, hooks=[self.predict_hooks.target_hook])
            preds = [pred[mode] if mode else pred for pred in preds]

        if self._predictions is not None:
            self._clear_prediction_queue()
        return self.input_pipeline.label_encoder.inverse_transform(np.asarray(preds))


    def predict_proba(self, X):
        """
        Produces a probability distribution over classes for each example in X.

        :param X: list or array of text to embed.
        :returns: list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.
        """
        return super().predict_proba(X)

    def finetune(self, X, Y=None, batch_size=None):
        """
        :param X: list or array of text.
        :param Y: integer or string-valued class labels.
        :param batch_size: integer number of examples per batch. When N_GPUS > 1, this number
                           corresponds to the number of training examples provided to each GPU.
        """
        raise NotImplementedError

    @classmethod
    def get_eval_fn(cls):
        raise NotImplementedError

    
    @staticmethod
    def _target_model(config, featurizer_state, targets, n_outputs, train=False, reuse=None, **kwargs):
        raise NotImplementedError

    def _predict_op(self, logits, **kwargs):
        raise NotImplementedError

    def _predict_proba_op(self, logits, **kwargs):
        raise NotImplementedError

    def fit(self, *args, **kwargs):
        raise NotImplementedError

    def attention_weights(self, Xs):
        raise NotImplementedError

    def generate_text(self, seed_text, max_length, use_extra_toks):
        raise NotImplementedError
    
    def save(self, path):
        raise NotImplementedError

    def create_base_model(self, filename, exists_ok):
        raise NotImplementedError
    
    def load(cls, path, **kwargs):
        raise NotImplementedError

    def finetune_grid_search(cls, Xs, Y, *, test_size, eval_fn, probs, return_all, **kwargs):
        raise NotImplementedError

    def finetune_grid_search_cv(cls, Xs, Y, *, n_splits, test_size, eval_fn, probs,
                                return_all, **kwargs):
        raise NotImplementedError

