

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Finetune Quickstart Guide &mdash; finetune 0.7.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="finetune 0.7.1 documentation" href="#"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> finetune
          

          
          </a>

          
            
            
              <div class="version">
                0.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Finetune Quickstart Guide</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
<li><a class="reference internal" href="#dataset-loading">Dataset Loading</a></li>
<li><a class="reference internal" href="#prediction">Prediction</a></li>
<li><a class="reference internal" href="#saving-and-loading-models">Saving and Loading Models</a></li>
<li><a class="reference internal" href="#using-different-base-models-e-g-bert-gpt2">Using Different Base Models (e.g. BERT, GPT2)</a></li>
<li><a class="reference internal" href="#using-adapters-and-the-deploymentmodel-class">Using Adapters and the DeploymentModel class</a></li>
<li><a class="reference internal" href="#code-examples">Code Examples</a></li>
<li><a class="reference internal" href="#finetune-api-reference">Finetune API Reference</a></li>
<li><a class="reference internal" href="#finetune-model-configuration-options">Finetune Model Configuration Options</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">finetune</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Finetune Quickstart Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="figure">
<img alt="https://i.imgur.com/kYL058E.png" src="https://i.imgur.com/kYL058E.png" />
</div>
<span class="target" id="module-finetune"></span><p><strong>Scikit-learn inspired model finetuning for natural language processing.</strong></p>
<p><a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> ships with a pre-trained language model from <a class="reference external" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">“Improving Language Understanding by Generative Pre-Training”</a>
and builds off the <a class="reference external" href="https://github.com/openai/finetune-transformer-lm">OpenAI/finetune-language-model repository</a>.</p>
<p>Source code for <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> is available <a class="reference external" href="https://github.com/IndicoDataSolutions/finetune">on github</a>.</p>
<div class="section" id="finetune-quickstart-guide">
<h1>Finetune Quickstart Guide<a class="headerlink" href="#finetune-quickstart-guide" title="Permalink to this headline">¶</a></h1>
<p>Finetuning the base language model is as easy as calling <a class="reference internal" href="#finetune.Classifier.fit" title="finetune.Classifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Classifier.fit()</span></code></a>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>               <span class="c1"># Load base model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">)</span>          <span class="c1"># Finetune base model on custom data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span> <span class="c1"># [{&#39;class_1&#39;: 0.23, &#39;class_2&#39;: 0.54, ..}, ..]</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>                   <span class="c1"># Serialize the model to disk</span>
</pre></div>
</div>
<p>Reload saved models from disk by using <a class="reference internal" href="#finetune.Classifier.load" title="finetune.Classifier.load"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Classifier.load()</span></code></a>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>Finetune can be installed directly from PyPI by using <cite>pip</cite></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install finetune
</pre></div>
</div>
<p>or installed directly from source:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/IndicoDataSolutions/finetune
<span class="nb">cd</span> finetune
python3 setup.py develop
python3 -m spacy download en
</pre></div>
</div>
<p>You can optionally run the provided test suite to ensure installation completed successfully.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip3 install pytest
pytest
</pre></div>
</div>
</div>
<div class="section" id="docker">
<h1>Docker<a class="headerlink" href="#docker" title="Permalink to this headline">¶</a></h1>
<p>If you’d prefer you can also run <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> in a docker container. The bash scripts provided assume you have a functional install of <a class="reference external" href="https://docs.docker.com/install">docker</a> and <a class="reference external" href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)">nvidia-docker</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># For usage with NVIDIA GPUs</span>
./docker/build_gpu_docker.sh  <span class="c1"># builds a docker image</span>
./docker/start_gpu_docker.sh  <span class="c1"># starts a docker container in the background</span>
docker <span class="nb">exec</span> -it finetune bash <span class="c1"># starts a bash session in the docker container</span>
</pre></div>
</div>
<p>For CPU-only usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./docker/build_cpu_docker.sh
./docker/start_cpu_docker.sh
</pre></div>
</div>
</div>
<div class="section" id="dataset-loading">
<h1>Dataset Loading<a class="headerlink" href="#dataset-loading" title="Permalink to this headline">¶</a></h1>
<p>Finetune supports providing input data as a list or as a data generator.  When a generator is provided as input, finetune
takes advantage of the <code class="xref py py-mod docutils literal notranslate"><span class="pre">tf.data</span></code> module for data pipelining</p>
<p>Providing text and targets in list format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;german shepherd&#39;</span><span class="p">,</span> <span class="s1">&#39;maine coon&#39;</span><span class="p">,</span> <span class="s1">&#39;persian&#39;</span><span class="p">,</span> <span class="s1">&#39;beagle&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Providing data as a generator:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;pets.csv&#39;</span><span class="p">)</span>

<span class="c1"># Even if raw data is greedily loaded,</span>
<span class="c1"># using a generator allows us to defer data preprocessing</span>
<span class="k">def</span> <span class="nf">text_generator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">Text</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">row</span><span class="o">.</span><span class="n">Text</span>

<span class="c1"># dataset_size must be specified if input is provided as generators</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">dataset_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_generator</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="prediction">
<h1>Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h1>
<p>After fitting the model, call <code class="xref py py-func docutils literal notranslate"><span class="pre">BaseModel.predict()</span></code> to infer on test data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</pre></div>
</div>
<p>To prevent recreating the tensorflow graph with each call to <code class="xref py py-func docutils literal notranslate"><span class="pre">BaseModel.predict()</span></code>,
use the <code class="xref py py-func docutils literal notranslate"><span class="pre">model.cached_predict()</span></code> context manager.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">cached_predict</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="c1"># triggers prediction graph construction</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="c1"># graph is already cached, so subsequence calls are faster</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-and-loading-models">
<h1>Saving and Loading Models<a class="headerlink" href="#saving-and-loading-models" title="Permalink to this headline">¶</a></h1>
<p>You can use the <code class="xref py py-func docutils literal notranslate"><span class="pre">BaseModel.save()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">load()</span></code> methods to serialize and deserialize trained models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-different-base-models-e-g-bert-gpt2">
<h1>Using Different Base Models (e.g. BERT, GPT2)<a class="headerlink" href="#using-different-base-models-e-g-bert-gpt2" title="Permalink to this headline">¶</a></h1>
<p>Finetune defaults to using OpenAI’s GPT base model, but also supports other base model options.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">finetune.base_models</span> <span class="kn">import</span> <span class="n">BERT</span><span class="p">,</span> <span class="n">BERTLarge</span><span class="p">,</span> <span class="n">GPT2</span><span class="p">,</span> <span class="n">GPT2Medium</span><span class="p">,</span> <span class="n">TextCNN</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">base_model</span><span class="o">=</span><span class="n">BERT</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-adapters-and-the-deploymentmodel-class">
<h1>Using Adapters and the DeploymentModel class<a class="headerlink" href="#using-adapters-and-the-deploymentmodel-class" title="Permalink to this headline">¶</a></h1>
<p>Alongside full finetuning, <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> also supports the adapter finetuning strategy from <a class="reference external" href="https://arxiv.org/abs/1902.00751">“Parameter-Efficient Transfer Learning for NLP”</a>.
This dramatically shrinks the size of serialized model files.  When used in conjunction with the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeploymentModel</span></code> class at inference time, this enables quickly switching between target models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we train and save a model using the adapter finetuning strategy</span>
<span class="kn">from</span> <span class="nn">finetune</span> <span class="kn">import</span> <span class="n">Classifier</span><span class="p">,</span> <span class="n">DeploymentModel</span>
<span class="kn">from</span> <span class="nn">finetune.base_models</span> <span class="kn">import</span> <span class="n">GPT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;adapter-model.jl&#39;</span><span class="p">)</span>

<span class="c1"># Then we load it using the DeploymentModel wrapper</span>
<span class="n">deployment_model</span> <span class="o">=</span> <span class="n">DeploymentModel</span><span class="p">(</span><span class="n">featurizer</span><span class="o">=</span><span class="n">GPT</span><span class="p">)</span>

<span class="c1"># Loading the featurizer only needs to be done once</span>
<span class="n">deployment_model</span><span class="o">.</span><span class="n">load_featurizer</span><span class="p">()</span>

<span class="c1"># You can then cheaply load + predict with any adapter model that uses the</span>
<span class="c1"># same base_model and adapter_size</span>
<span class="n">deployment_model</span><span class="o">.</span><span class="n">load_custom_model</span><span class="p">(</span><span class="s1">&#39;adapter-model.jl&#39;</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">deployment_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>

<span class="c1"># Switching to another model takes only 2 seconds now rather than 20</span>
<span class="n">deployment_model</span><span class="o">.</span><span class="n">load_custom_model</span><span class="p">(</span><span class="s1">&#39;another-adapter-model.jl&#39;</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">deployment_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">textX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="code-examples">
<h1>Code Examples<a class="headerlink" href="#code-examples" title="Permalink to this headline">¶</a></h1>
<p>For example usage of provided models, see the <a class="reference external" href="https://github.com/IndicoDataSolutions/finetune/tree/master/finetune/datasets">finetune/datasets directory</a>.</p>
</div>
<div class="section" id="finetune-api-reference">
<h1>Finetune API Reference<a class="headerlink" href="#finetune-api-reference" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="finetune.Classifier">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Classifier</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a single document into 1 of N categories.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Classifier.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – integer or string-valued class labels.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Classifier.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Classifier.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Classifier.save" title="finetune.Classifier.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/classifier.html#Classifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Classifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Classifier.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Classifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.Regressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Regressor</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Regresses one or more floating point values given a single document.</p>
<p>For a full list of configuration options, see <cite>finetune.config</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A config object generated by <cite>finetune.config.get_config</cite> or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Regressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Regressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Regressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Regressor.save" title="finetune.Regressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/regressor.html#Regressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Regressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Regressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Regressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.MultiFieldClassifier">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiFieldClassifier</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a set of documents into 1 of N classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiFieldClassifier.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</li>
<li><strong>Y</strong> – integer or string-valued class labels. It is necessary for the items of Y to be sortable.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldClassifier.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldClassifier.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiFieldClassifier.save" title="finetune.MultiFieldClassifier.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from X2 class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldClassifier.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldClassifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.MultiFieldRegressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiFieldRegressor</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Regresses one or more floating point values given a set of documents per example.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiFieldRegressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldRegressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiFieldRegressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiFieldRegressor.save" title="finetune.MultiFieldRegressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>Xs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multifield.html#MultiFieldRegressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiFieldRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*Xs</strong> – lists of text inputs, shape [batch, n_fields]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from X2 class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiFieldRegressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiFieldRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.MultiLabelClassifier">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiLabelClassifier</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a single document into upto N of N categories.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiLabelClassifier.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – A list of lists containing labels for the corresponding X</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiLabelClassifier.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiLabelClassifier.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiLabelClassifier.save" title="finetune.MultiLabelClassifier.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>threshold=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/multi_label_classifier.html#MultiLabelClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiLabelClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiLabelClassifier.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiLabelClassifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.SequenceLabeler">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">SequenceLabeler</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler" title="Permalink to this definition">¶</a></dt>
<dd><p>Labels each token in a sequence as belonging to 1 of N token classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.SequenceLabeler.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>Xs</strong> – An iterable of lists or array of text, shape [batch, n_inputs, tokens]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.SequenceLabeler.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.SequenceLabeler.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.SequenceLabeler.save" title="finetune.SequenceLabeler.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>per_token=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> – A list / array of text, shape [batch]</li>
<li><strong>per_token</strong> – If True, return raw probabilities and labels on a per token basis</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">list of class labels.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/sequence_labeling.html#SequenceLabeler.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.SequenceLabeler.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A list / array of text, shape [batch]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.SequenceLabeler.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.SequenceLabeler.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.Comparison">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">Comparison</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison" title="Permalink to this definition">¶</a></dt>
<dd><p>Compares two documents to solve a classification task.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A <a class="reference internal" href="#finetune.config.Settings" title="finetune.config.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">finetune.config.Settings</span></code></a> object or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.Comparison.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – integer or string-valued class labels.</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Comparison.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.Comparison.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.Comparison.save" title="finetune.Comparison.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/comparison.html#Comparison.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.Comparison.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pairs</strong> – Array of text, shape [batch, 2]</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.Comparison.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.Comparison.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.OrdinalRegressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">OrdinalRegressor</code><span class="sig-paren">(</span><em>shared_threshold_weights=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Classifies a document into two or more ordered categories.</p>
<p>For a full list of configuration options, see <cite>finetune.config</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A config object generated by <cite>finetune.config.get_config</cite> or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.OrdinalRegressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.OrdinalRegressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.OrdinalRegressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.OrdinalRegressor.save" title="finetune.OrdinalRegressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#OrdinalRegressor.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.OrdinalRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.OrdinalRegressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.OrdinalRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.ComparisonOrdinalRegressor">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">ComparisonOrdinalRegressor</code><span class="sig-paren">(</span><em>shared_threshold_weights=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/ordinal_regressor.html#ComparisonOrdinalRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Compares two documents and classifies into two or more ordered categories.</p>
<p>For a full list of configuration options, see <cite>finetune.config</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>config</strong> – A config object generated by <cite>finetune.config.get_config</cite> or None (for default config).</li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Embeds inputs in learned feature space. Can be called before or after calling <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetune()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">np.array of features of shape (n_examples, embedding_size).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> – list or array of text.</li>
<li><strong>Y</strong> – floating point targets</li>
<li><strong>batch_size</strong> – integer number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.ComparisonOrdinalRegressor.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.ComparisonOrdinalRegressor.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.ComparisonOrdinalRegressor.save" title="finetune.ComparisonOrdinalRegressor.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a list of most likely class labels as determined by the fine-tuned model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of class labels.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Produces a probability distribution over classes for each example in X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – list or array of text to embed.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of dictionaries.  Each dictionary maps from a class label to its assigned class probability.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.ComparisonOrdinalRegressor.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.ComparisonOrdinalRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="finetune.MultiTask">
<em class="property">class </em><code class="descclassname">finetune.</code><code class="descname">MultiTask</code><span class="sig-paren">(</span><em>tasks</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask" title="Permalink to this definition">¶</a></dt>
<dd><p>Target model for multi task learning. The approach used is to sample mini-batches from each task proportional to
the size of the task for each dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tasks</strong> – A dictionary of pairs mapping string task names to model classes.
eg. <cite>{“sst”: Classifier, “ner”: SequenceLabeler}</cite></li>
<li><strong>**kwargs</strong> – key-value pairs of config items to override. Note: The same config is used for each base task.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="finetune.MultiTask.cached_predict">
<code class="descname">cached_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.cached_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.cached_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that prevents the recreation of the tensorflow graph on every call to BaseModel.predict().</p>
<p>Not supported for MultiTask.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.create_base_model">
<code class="descname">create_base_model</code><span class="sig-paren">(</span><em>filename</em>, <em>exists_ok=False</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.create_base_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the current weights into the correct file format to be used as a base model.
:param filename: the path to save the base model relative to finetune’s base model filestore.
:param exists_ok: Whether to replace the model if it exists.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.featurize">
<code class="descname">featurize</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.featurize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.featurize" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs featurization on the trained model for any of the tasks the model was trained for. Input and output formats  
are the same as for each of the individial tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A dictionary mapping from task name to data, in the format required by the task type.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A dictionary mapping from task name to the features for that task.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.finetune">
<code class="descname">finetune</code><span class="sig-paren">(</span><em>X</em>, <em>Y=None</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.finetune"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.finetune" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> – A dictionary mapping from task name to inputs in the same format required for each of the models.</li>
<li><strong>Y</strong> – A dictionary mapping from task name to targets in the same format required for each of the models.</li>
<li><strong>batch_size</strong> – Number of examples per batch. When N_GPUS &gt; 1, this number
corresponds to the number of training examples provided to each GPU.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiTask.finetune_grid_search">
<em class="property">classmethod </em><code class="descname">finetune_grid_search</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.finetune_grid_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 inputs (prediction, truth) and returns a float, with a max value being desired.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="finetune.MultiTask.finetune_grid_search_cv">
<em class="property">classmethod </em><code class="descname">finetune_grid_search_cv</code><span class="sig-paren">(</span><em>Xs</em>, <em>Y</em>, <em>*</em>, <em>n_splits</em>, <em>test_size</em>, <em>eval_fn=None</em>, <em>probs=False</em>, <em>return_all=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.finetune_grid_search_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs cross validated grid search over config items defined using “GridSearchable” objects and returns either full results or
the config object that relates to the best results. The default config contains grid searchable objects for the
most important parameters to search over.</p>
<p>It should be noted that the cv splits are not guaranteed unique, but each split is given to each set of hparams.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>Xs</strong> – Input text. Either [num_samples] or [sequence, num_samples] for single or multi input models respectively.</li>
<li><strong>Y</strong> – Targets, A list of targets, [num_samples] that correspond to each sample in Xs.</li>
<li><strong>n_splits</strong> – Number of CV splits to do.</li>
<li><strong>test_size</strong> – Int or float. If an int is given this number of samples is used to validate, if a float is
given then that fraction of samples is used.</li>
<li><strong>eval_fn</strong> – An eval function that takes 2 batches of outputs and returns a float, with a max value being
desired. An arithmetic mean must make sense for this metric.</li>
<li><strong>probs</strong> – If true, eval_fn is passed probability outputs from predict_proba, otherwise the output of predict is used.</li>
<li><strong>return_all</strong> – If True, all results are returned, if False, only the best config is returned.</li>
<li><strong>kwargs</strong> – Keyword arguments to pass to get_config()</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">default is to return the best config object. If return_all is true, it returns a list of tuples of the
form [(config, eval_fn output), … ]</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for finetune.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.generate_text">
<code class="descname">generate_text</code><span class="sig-paren">(</span><em>seed_text=''</em>, <em>max_length=None</em>, <em>use_extra_toks=True</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.generate_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a prediction on the Language modeling objective given some seed text. It uses a noisy greedy decoding.
Temperature parameter for decoding is set in the config.
:param max_length: The maximum length to decode to.
:param seed_text: Defaults to the empty string. This will form the starting point to begin modelling
:return: A string containing the generated text.</p>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved fine-tuned model from disk.  Path provided should be a folder which contains .pkl and tf.Saver() files</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> – string path name to load model from.  Same value as previously provided to <a class="reference internal" href="#finetune.MultiTask.save" title="finetune.MultiTask.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>. Must be a folder.</li>
<li><strong>**kwargs</strong> – <p>key-value pairs of config items to override.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs inference on the trained model for any of the tasks the model was trained for. Input and output formats
are the same as for each of the individial tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A dictionary mapping from task name to data, in the format required by the task type.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A dictionary mapping from task name to the predictions for that task.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/target_models/mtl.html#MultiTask.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.MultiTask.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs probability inference on the trained model for any of the tasks the model was trained for. Falls back
to normal predict when probabilities are not available for a task, eg Regression.</p>
<p>Input and output formats are the same as for each of the individial tasks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> – A dictionary mapping from task name to data, in the format required by the task type.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A dictionary mapping from task name to the predictions for that task.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the state of the model to disk to the folder specific by <cite>path</cite>.  If <cite>path</cite> does not exist, it will be auto-created.</p>
<dl class="docutils">
<dt>Save is performed in two steps:</dt>
<dd><ul class="first last simple">
<li>Serialize tf graph to disk using tf.Saver</li>
<li>Serialize python model using pickle</li>
</ul>
</dd>
<dt>Note:</dt>
<dd>Does not serialize state of Adam optimizer.
Should not be used to save / restore a training model.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="finetune.MultiTask.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#finetune.MultiTask.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>An alias for <cite>featurize</cite>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="finetune-model-configuration-options">
<h1>Finetune Model Configuration Options<a class="headerlink" href="#finetune-model-configuration-options" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="finetune.config.Settings">
<em class="property">class </em><code class="descclassname">finetune.config.</code><code class="descname">Settings</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/finetune/config.html#Settings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#finetune.config.Settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Model configuration options</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>batch_size</strong> – Number of examples per batch, defaults to <cite>2</cite>.</li>
<li><strong>visible_gpus</strong> – List of integer GPU ids to spread out computation across, defaults to all available GPUs.</li>
<li><strong>n_epochs</strong> – Number of iterations through training data, defaults to <cite>3</cite>.</li>
<li><strong>random_seed</strong> – Random seed to use for repeatability purposes, defaults to <cite>42</cite>.</li>
<li><strong>max_length</strong> – Maximum number of subtokens per sequence. Examples longer than this number will be truncated
(unless <cite>chunk_long_sequences=True</cite> for SequenceLabeler models). Defaults to <cite>512</cite>.</li>
<li><strong>weight_stddev</strong> – Standard deviation of initial weights.  Defaults to <cite>0.02</cite>.</li>
<li><strong>chunk_long_sequences</strong> – When True, use a sliding window approach to predict on
examples that are longer than max length.  Defaults to <cite>False</cite>.</li>
<li><strong>low_memory_mode</strong> – When True, only store partial gradients on forward pass
and recompute remaining gradients incrementally in order to save memory.  Defaults to <cite>False</cite>.</li>
<li><strong>interpolate_pos_embed</strong> – Interpolate positional embeddings when <cite>max_length</cite> differs from it’s original value of
<cite>512</cite>. Defaults to <cite>False</cite>.</li>
<li><strong>embed_p_drop</strong> – Embedding dropout probability.  Defaults to <cite>0.1</cite>.</li>
<li><strong>attn_p_drop</strong> – Attention dropout probability.  Defaults to <cite>0.1</cite>.</li>
<li><strong>resid_p_drop</strong> – Residual layer fully connected network dropout probability.  Defaults to <cite>0.1</cite>.</li>
<li><strong>clf_p_drop</strong> – Classifier dropout probability.  Defaults to <cite>0.1</cite>.</li>
<li><strong>l2_reg</strong> – L2 regularization coefficient. Defaults to <cite>0.01</cite>.</li>
<li><strong>vector_l2</strong> – Whether to apply weight decay regularization to vectors (biases, normalization etc..). Defaults to False.</li>
<li><strong>optimizer</strong> – Optimizer to use, current options include AdamW or AdamaxW.</li>
<li><strong>b1</strong> – Adam b1 parameter.  Defaults to <cite>0.9</cite>.</li>
<li><strong>b2</strong> – Adam b2 parameter.  Defaults to <cite>0.999</cite>.</li>
<li><strong>epsilon</strong> – Adam epsilon parameter: Defaults to <cite>1e-8</cite>.</li>
<li><strong>lr_schedule</strong> – Learning rate schedule – see <cite>finetune/optimizers.py</cite> for more options.</li>
<li><strong>lr</strong> – Learning rate.  Defaults to <cite>6.25e-5</cite>.</li>
<li><strong>lr_warmup</strong> – Learning rate warmup (percentage of all batches to warmup for).  Defaults to <cite>0.002</cite>.</li>
<li><strong>max_grad_norm</strong> – Clip gradients larger than this norm. Defaults to <cite>1.0</cite>.</li>
<li><strong>accum_steps</strong> – Number of updates to accumulate before applying. This is used to simulate a higher batch size.</li>
<li><strong>lm_loss_coef</strong> – Language modeling loss coefficient – a value between <cite>0.0</cite> - <cite>1.0</cite>
that indicates how to trade off between language modeling loss
and target model loss.  Usually not beneficial to turn on unless
dataset size exceeds a few thousand examples.  Defaults to <cite>0.0</cite>.</li>
<li><strong>summarize_grads</strong> – Include gradient summary information in tensorboard.  Defaults to <cite>False</cite>.</li>
<li><strong>val_size</strong> – Validation set size if int. Validation set size as percentage of all training data if float.  Validation will not be run by default if n_examples &lt; 50.
If n_examples &gt; 50, defaults to max(5, min(100, 0.05 * n_examples))</li>
<li><strong>val_interval</strong> – Evaluate on validation set after <cite>val_interval</cite> batches.
Defaults to 4 * val_size / batch_size to ensure that too much time is not spent on validation.</li>
<li><strong>lm_temp</strong> – Language model temperature – a value of <cite>0.0</cite> corresponds to greedy maximum likelihood predictions
while a value of <cite>1.0</cite> corresponds to random predictions. Defaults to <cite>0.2</cite>.</li>
<li><strong>seq_num_heads</strong> – Number of attention heads of final attention layer. Defaults to <cite>16</cite>.</li>
<li><strong>subtoken_predictions</strong> – Return predictions at subtoken granularity or token granularity?  Defaults to <cite>False</cite>.</li>
<li><strong>multi_label_sequences</strong> – Use a multi-labeling approach to sequence labeling to allow overlapping labels.</li>
<li><strong>multi_label_threshold</strong> – Threshold of sigmoid unit in multi label classifier.
Can be increased or lowered to trade off precision / recall. Defaults to <cite>0.5</cite>.</li>
<li><strong>autosave_path</strong> – Save current best model (as measured by validation loss) to this location. Defaults to <cite>None</cite>.</li>
<li><strong>tensorboard_folder</strong> – Directory for tensorboard logs. Tensorboard logs will not be written
unless tensorboard_folder is explicitly provided. Defaults to <cite>None</cite>.</li>
<li><strong>log_device_placement</strong> – Log which device each operation is placed on for debugging purposes.  Defaults to <cite>False</cite>.</li>
<li><strong>allow_soft_placement</strong> – Allow tf to allocate an operation to a different device if a device is unavailable.  Defaults to <cite>True</cite>.</li>
<li><strong>save_adam_vars</strong> – Save adam parameters when calling <cite>model.save()</cite>.  Defaults to <cite>True</cite>.</li>
<li><strong>num_layers_trained</strong> – How many layers to finetune.  Specifying a value less than 12 will train layers starting from model output. Defaults to <cite>12</cite>.</li>
<li><strong>train_embeddings</strong> – Should embedding layer be finetuned? Defaults to <cite>True</cite>.</li>
<li><strong>class_weights</strong> – One of ‘log’, ‘linear’, or ‘sqrt’. Auto-scales gradient updates based on class frequency.  Can also be a dictionary that maps from true class name to loss coefficient. Defaults to <cite>None</cite>.</li>
<li><strong>oversample</strong> – Should rare classes be oversampled?  Defaults to <cite>False</cite>.</li>
<li><strong>params_device</strong> – Which device should gradient updates be aggregated on?
If you are using a single GPU and have more than 4Gb of GPU memory you should set this to GPU PCI number (0, 1, 2, etc.). Defaults to <cite>“cpu”</cite>.</li>
<li><strong>eval_acc</strong> – if True, calculates accuracy and writes it to the tensorboard summary files for valudation runs.</li>
<li><strong>save_dtype</strong> – specifies what precision to save model weights with.  Defaults to <cite>np.float32</cite>.</li>
<li><strong>regression_loss</strong> – the loss to use for regression models. One of <cite>L1</cite> or <cite>L2</cite>, defaults to <cite>L2</cite>.</li>
<li><strong>prefit_init</strong> – if True, fit target model weigths before finetuning the entire model. Defaults to <cite>False</cite>.</li>
<li><strong>debugging_logs</strong> – if True, output tensorflow logs and turn off TQDM logging. Defaults to <cite>False</cite>.</li>
<li><strong>val_set</strong> – Where it is neccessary to use an explicit validation set, provide it here as a tuple (text, labels)</li>
<li><strong>per_process_gpu_memory_fraction</strong> – fraction of the overall amount of memory that each visible GPU should be allocated, defaults to <cite>1.0</cite>.</li>
<li><strong>adapter_size</strong> – width of adapter module from ‘Parameter Efficient Transfer Learning’ paper, if defined. defaults to ‘None’.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Madison May, Ben Townsend.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.7.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>