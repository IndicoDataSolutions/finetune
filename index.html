

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Finetune Quickstart Guide &mdash; finetune 0.7.2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="finetune 0.7.2 documentation" href="#"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> finetune
          

          
          </a>

          
            
            
              <div class="version">
                0.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Finetune Quickstart Guide</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
<li><a class="reference internal" href="#dataset-loading">Dataset Loading</a></li>
<li><a class="reference internal" href="#prediction">Prediction</a></li>
<li><a class="reference internal" href="#saving-and-loading-models">Saving and Loading Models</a></li>
<li><a class="reference internal" href="#using-different-base-models-e-g-bert-gpt2-roberta">Using Different Base Models (e.g. BERT, GPT2, RoBERTa)</a></li>
<li><a class="reference internal" href="#using-the-sequencelabeler-class">Using the SequenceLabeler Class</a></li>
<li><a class="reference internal" href="#using-adapters-and-the-deploymentmodel-class">Using Adapters and the DeploymentModel class</a></li>
<li><a class="reference internal" href="#using-auxiliary-info-in-your-models">Using Auxiliary Info in Your Models</a></li>
<li><a class="reference internal" href="#code-examples">Code Examples</a></li>
<li><a class="reference internal" href="#finetune-api-reference">Finetune API Reference</a></li>
<li><a class="reference internal" href="#finetune-model-configuration-options">Finetune Model Configuration Options</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">finetune</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Finetune Quickstart Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="figure">
<img alt="https://i.imgur.com/kYL058E.png" src="https://i.imgur.com/kYL058E.png" />
</div>
<span class="target" id="module-finetune"></span><p><strong>Scikit-learn inspired model finetuning for natural language processing.</strong></p>
<p><a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> ships with a pre-trained language model from <a class="reference external" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">“Improving Language Understanding by Generative Pre-Training”</a>
and builds off the <a class="reference external" href="https://github.com/openai/finetune-transformer-lm">OpenAI/finetune-language-model repository</a>.</p>
<p>Source code for <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> is available <a class="reference external" href="https://github.com/IndicoDataSolutions/finetune">on github</a>.</p>
<div class="section" id="finetune-quickstart-guide">
<h1>Finetune Quickstart Guide<a class="headerlink" href="#finetune-quickstart-guide" title="Permalink to this headline">¶</a></h1>
<p>Finetuning the base language model is as easy as calling <code class="xref py py-meth docutils literal notranslate"><span class="pre">Classifier.fit()</span></code>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>               <span class="c1"># Load base model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">)</span>          <span class="c1"># Finetune base model on custom data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span> <span class="c1"># [{&#39;class_1&#39;: 0.23, &#39;class_2&#39;: 0.54, ..}, ..]</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>                   <span class="c1"># Serialize the model to disk</span>
</pre></div>
</div>
<p>Reload saved models from disk by using <code class="xref py py-meth docutils literal notranslate"><span class="pre">Classifier.load()</span></code>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>Finetune can be installed directly from PyPI by using <cite>pip</cite></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install finetune
</pre></div>
</div>
<p>or installed directly from source:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/IndicoDataSolutions/finetune
<span class="nb">cd</span> finetune
python3 setup.py develop
python3 -m spacy download en
</pre></div>
</div>
<p>You can optionally run the provided test suite to ensure installation completed successfully.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip3 install pytest
pytest
</pre></div>
</div>
</div>
<div class="section" id="docker">
<h1>Docker<a class="headerlink" href="#docker" title="Permalink to this headline">¶</a></h1>
<p>If you’d prefer you can also run <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> in a docker container. The bash scripts provided assume you have a functional install of <a class="reference external" href="https://docs.docker.com/install">docker</a> and <a class="reference external" href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)">nvidia-docker</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># For usage with NVIDIA GPUs</span>
./docker/build_gpu_docker.sh  <span class="c1"># builds a docker image</span>
./docker/start_gpu_docker.sh  <span class="c1"># starts a docker container in the background</span>
docker <span class="nb">exec</span> -it finetune bash <span class="c1"># starts a bash session in the docker container</span>
</pre></div>
</div>
<p>For CPU-only usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./docker/build_cpu_docker.sh
./docker/start_cpu_docker.sh
</pre></div>
</div>
</div>
<div class="section" id="dataset-loading">
<h1>Dataset Loading<a class="headerlink" href="#dataset-loading" title="Permalink to this headline">¶</a></h1>
<p>Finetune supports providing input data as a list or as a data generator.  When a generator is provided as input, finetune
takes advantage of the <code class="xref py py-mod docutils literal notranslate"><span class="pre">tf.data</span></code> module for data pipelining</p>
<p>Providing text and targets in list format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;german shepherd&#39;</span><span class="p">,</span> <span class="s1">&#39;maine coon&#39;</span><span class="p">,</span> <span class="s1">&#39;persian&#39;</span><span class="p">,</span> <span class="s1">&#39;beagle&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Providing data as a generator:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;pets.csv&#39;</span><span class="p">)</span>

<span class="c1"># Even if raw data is greedily loaded,</span>
<span class="c1"># using a generator allows us to defer data preprocessing</span>
<span class="k">def</span> <span class="nf">text_generator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">Text</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">row</span><span class="o">.</span><span class="n">Text</span>

<span class="c1"># dataset_size must be specified if input is provided as generators</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">dataset_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_generator</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="prediction">
<h1>Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h1>
<p>After fitting the model, call <code class="xref py py-func docutils literal notranslate"><span class="pre">BaseModel.predict()</span></code> to infer on test data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</pre></div>
</div>
<p>To prevent recreating the tensorflow graph with each call to <code class="xref py py-func docutils literal notranslate"><span class="pre">BaseModel.predict()</span></code>,
use the <code class="xref py py-func docutils literal notranslate"><span class="pre">model.cached_predict()</span></code> context manager.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">cached_predict</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="c1"># triggers prediction graph construction</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="c1"># graph is already cached, so subsequence calls are faster</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-and-loading-models">
<h1>Saving and Loading Models<a class="headerlink" href="#saving-and-loading-models" title="Permalink to this headline">¶</a></h1>
<p>You can use the <code class="xref py py-func docutils literal notranslate"><span class="pre">BaseModel.save()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">load()</span></code> methods to serialize and deserialize trained models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-different-base-models-e-g-bert-gpt2-roberta">
<h1>Using Different Base Models (e.g. BERT, GPT2, RoBERTa)<a class="headerlink" href="#using-different-base-models-e-g-bert-gpt2-roberta" title="Permalink to this headline">¶</a></h1>
<p>Finetune defaults to using OpenAI’s GPT base model, but also supports other base model options.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">finetune.base_models</span> <span class="kn">import</span> <span class="n">BERT</span><span class="p">,</span> <span class="n">BERTLarge</span><span class="p">,</span> <span class="n">GPT2</span><span class="p">,</span> <span class="n">GPT2Medium</span><span class="p">,</span> <span class="n">TextCNN</span><span class="p">,</span> <span class="n">RoBERTa</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">base_model</span><span class="o">=</span><span class="n">RoBERTa</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-the-sequencelabeler-class">
<h1>Using the SequenceLabeler Class<a class="headerlink" href="#using-the-sequencelabeler-class" title="Permalink to this headline">¶</a></h1>
<p>One of the dozen tasks our base models support is sequence labeling, where you label certain spans of text within a document rather than classifying the entire example. Labels for training
the SequenceLabeler are in the following format, as a list of lists of dictionaries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We include text, label, and start and end positions in our Y values. You do not need to create dictionaries for spans that have no label.</span>
<span class="c1"># The text in the &#39;text&#39; field must be equivalent to example[label[&#39;start&#39;]:label[&#39;end&#39;]]</span>
<span class="n">trainX</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Intelligent process automation&#39;</span><span class="p">]</span>
<span class="n">trainY</span> <span class="o">=</span> <span class="p">[[</span>
    <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;Intelligent&#39;</span><span class="p">,</span> <span class="s1">&#39;capitalized&#39;</span><span class="p">:</span> <span class="s1">&#39;True&#39;</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="s1">&#39;start&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;part_of_speech&#39;</span><span class="p">:</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;process automation&#39;</span><span class="p">,</span> <span class="s1">&#39;start&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span> <span class="s1">&#39;part_of_speech&#39;</span><span class="p">:</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">},</span>
<span class="p">]]</span>

<span class="kn">from</span> <span class="nn">finetune</span> <span class="kn">import</span> <span class="n">SequenceLabeler</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SequenceLabeler</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">)</span>

<span class="c1"># Prediction outputs are in the same format as labels</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">trainX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-adapters-and-the-deploymentmodel-class">
<h1>Using Adapters and the DeploymentModel class<a class="headerlink" href="#using-adapters-and-the-deploymentmodel-class" title="Permalink to this headline">¶</a></h1>
<p>Alongside full finetuning, <a class="reference internal" href="#module-finetune" title="finetune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">finetune</span></code></a> also supports the adapter finetuning strategy from <a class="reference external" href="https://arxiv.org/abs/1902.00751">“Parameter-Efficient Transfer Learning for NLP”</a>.
This dramatically shrinks the size of serialized model files.  When used in conjunction with the <code class="xref py py-class docutils literal notranslate"><span class="pre">DeploymentModel</span></code> class at inference time, this enables quickly switching between target models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we train and save a model using the adapter finetuning strategy</span>
<span class="kn">from</span> <span class="nn">finetune</span> <span class="kn">import</span> <span class="n">Classifier</span><span class="p">,</span> <span class="n">DeploymentModel</span>
<span class="kn">from</span> <span class="nn">finetune.base_models</span> <span class="kn">import</span> <span class="n">GPT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;adapter-model.jl&#39;</span><span class="p">)</span>

<span class="c1"># Then we load it using the DeploymentModel wrapper</span>
<span class="n">deployment_model</span> <span class="o">=</span> <span class="n">DeploymentModel</span><span class="p">(</span><span class="n">featurizer</span><span class="o">=</span><span class="n">GPT</span><span class="p">)</span>

<span class="c1"># Loading the featurizer only needs to be done once</span>
<span class="n">deployment_model</span><span class="o">.</span><span class="n">load_featurizer</span><span class="p">()</span>

<span class="c1"># You can then cheaply load + predict with any adapter model that uses the</span>
<span class="c1"># same base_model and adapter_size</span>
<span class="n">deployment_model</span><span class="o">.</span><span class="n">load_custom_model</span><span class="p">(</span><span class="s1">&#39;adapter-model.jl&#39;</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">deployment_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>

<span class="c1"># Switching to another model takes only 2 seconds now rather than 20</span>
<span class="n">deployment_model</span><span class="o">.</span><span class="n">load_custom_model</span><span class="p">(</span><span class="s1">&#39;another-adapter-model.jl&#39;</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">deployment_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">textX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-auxiliary-info-in-your-models">
<h1>Using Auxiliary Info in Your Models<a class="headerlink" href="#using-auxiliary-info-in-your-models" title="Permalink to this headline">¶</a></h1>
<p>Our base models can also process arbitrary auxiliary information in addition to text, such as style (bolding, italics, etc.), semantics (part-of-speech tags, sentiment tags), or other forms,
as long as they describe specific spans of text.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we define the extra features we will be providing, as well as a default value that it will take if given data does not cover the text.</span>
<span class="c1"># Auxiliary info can take the form of strings, booleans, floats, or ints.</span>
<span class="n">default</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;capitalized&#39;</span><span class="p">:</span><span class="bp">False</span><span class="p">,</span> <span class="s1">&#39;part_of_speech&#39;</span><span class="p">:</span><span class="s1">&#39;unknown&#39;</span><span class="p">}</span>

<span class="c1"># Next we create context tags in a similar format to SequenceLabeling labels, as a list of lists of dictionaries:</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Intelligent process automation&#39;</span><span class="p">]</span>
<span class="n">train_context</span> <span class="o">=</span> <span class="p">[[</span>
    <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;Intelligent&#39;</span><span class="p">,</span> <span class="s1">&#39;capitalized&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="s1">&#39;start&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;part_of_speech&#39;</span><span class="p">:</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;process automation&#39;</span><span class="p">,</span> <span class="s1">&#39;capitalized&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span> <span class="s1">&#39;start&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s1">&#39;part_of_speech&#39;</span><span class="p">:</span> <span class="s1">&#39;NOUN&#39;</span><span class="p">},</span>
<span class="p">]]</span>

<span class="c1"># Our input to the model is now a list containing the text, and then the context</span>
<span class="n">trainX</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_text</span><span class="p">,</span> <span class="n">train_context</span><span class="p">]</span>

<span class="c1"># We indicate to the model that we are including auxiliary info by passing our default dictionary in with the kwarg default_context.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">default_context</span><span class="o">=</span><span class="n">default</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="code-examples">
<h1>Code Examples<a class="headerlink" href="#code-examples" title="Permalink to this headline">¶</a></h1>
<p>For example usage of provided models, see the <a class="reference external" href="https://github.com/IndicoDataSolutions/finetune/tree/master/finetune/datasets">finetune/datasets directory</a>.</p>
</div>
<div class="section" id="finetune-api-reference">
<h1>Finetune API Reference<a class="headerlink" href="#finetune-api-reference" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="finetune-model-configuration-options">
<h1>Finetune Model Configuration Options<a class="headerlink" href="#finetune-model-configuration-options" title="Permalink to this headline">¶</a></h1>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Madison May, Ben Townsend.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.7.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>